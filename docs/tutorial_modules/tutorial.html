

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>The Data Driven Newsvendor &mdash; ddop  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Tutorial" href="../tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> ddop
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorial.html">Tutorial</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">The Data Driven Newsvendor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Getting-started">Getting started</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-Newsvendor-Problem-at-Yaz">The Newsvendor Problem at Yaz</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Data-and-Pre-processing">Data and Pre-processing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Parametric-Approach">Parametric Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Data-Driven-Approaches">Data-Driven Approaches</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Sample-Average-Approximation">Sample Average Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Weighted-Sample-Average-Approximation">Weighted Sample Average Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Empirical-Risk-Minimization">Empirical Risk Minimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#A-Final-Note">A Final Note</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ddop</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../tutorial.html">Tutorial</a> &raquo;</li>
        
      <li>The Data Driven Newsvendor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial_modules/tutorial.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="The-Data-Driven-Newsvendor">
<h1>The Data Driven Newsvendor<a class="headerlink" href="#The-Data-Driven-Newsvendor" title="Permalink to this headline">¶</a></h1>
<p>The newsvendor problem is the classical single period inventory problem that refers to a situation in which a seller (the newsvendor) has to determine the order quantity of perishable goods for the next selling period under uncertainty in demand. The traditional way to solve the problem assumes that the demand distribution is known. In practice, however, we almost never know the true demand distribution.</p>
<p>In the following tutorial, you will get to know different approaches to solve the newsvendor problem when the underlying demand distribution is unknown but the decision maker has access to past demand observations. In this context, we consider the decision problem of Yaz, a fast-casual restaurant in Stuttgart. In addition, you will learn how to apply these approaches using our Python library <code class="docutils literal notranslate"><span class="pre">ddop</span></code>.</p>
<div class="section" id="Getting-started">
<h2>Getting started<a class="headerlink" href="#Getting-started" title="Permalink to this headline">¶</a></h2>
<p>Before jumping into the tutorial, you should know the basics of Python and be familiar with well known libraries like numpy, pandas, and scikit-learn. To execute code, make sure you have an empty Python virtual environment installed on your computer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pip install ddop==0.6.3 seaborn==0.10.1 matplotlib
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from ddop.metrics import average_costs
from ddop.newsvendor import SampleAverageApproximationNewsvendor
from ddop.newsvendor import DecisionTreeWeightedNewsvendor
from ddop.newsvendor import KNeighborsWeightedNewsvendor
from ddop.newsvendor import LinearRegressionNewsvendor
from ddop.newsvendor import DeepLearningNewsvendor
from scipy.stats import norm
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Using TensorFlow backend.
</pre></div></div>
</div>
</div>
<div class="section" id="The-Newsvendor-Problem-at-Yaz">
<h2>The Newsvendor Problem at Yaz<a class="headerlink" href="#The-Newsvendor-Problem-at-Yaz" title="Permalink to this headline">¶</a></h2>
<p>Let us now start by introducing the decision problem at Yaz. Yaz is a fast casual restaurant in Stuttgart providing great oriental cuisine. The main ingredients for the meals, such as steak, lamb, fish, etc., are prepared at a central factory and are deep-frozen to achieve longer shelf lives. Depending on the estimated demand for the next day, the restaurant manager has to decide how much of the ingredients to defrost over night. These defrosted ingredients/meals then have to be sold within the
following day. If the defrosted quantity was too low, each unit of demand that cannot be satisfied incurs underage cost of <span class="math notranslate nohighlight">\(cu\)</span>. On the other hand, if the quantity was too high, unsold ingredients must be disposed of at overage cost of <span class="math notranslate nohighlight">\(co\)</span>. Therefore, the store manager wants to choose the order quantity that minimizes the sum of the expected costs.</p>
<p><img alt="image1" src="https://drive.google.com/uc?export=view&amp;id=1UPRyUC56wMVd554iHsvOks-MBJaXYwMf" /></p>
<p>More formally, the problem that the store manager is trying to solve is given by:</p>
<p><span class="math">\begin{equation}
\min_{q\geq 0} = E_D[cu(D-q)^+ + co(q-D)^+],
\tag{1}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the uncertain demand, <span class="math notranslate nohighlight">\(q\)</span> is the order quantity, <span class="math notranslate nohighlight">\(cu\)</span> and <span class="math notranslate nohighlight">\(co\)</span> are the per-unit under and overage costs, and <span class="math notranslate nohighlight">\((\cdot)^+ := \max\{0,\cdot\}\)</span> is a function that returns 0 if its argument is negative, and else its argument. The optimization problem at hand is what is known as the newsvendor problem. If the demand distribution is known, then the optimal decision can be calculated as follows:</p>
<p><span class="math">\begin{equation}
q^*=F^{-1}\biggl(\frac{cu}{cu+co}\biggl)=F^{-1}(\alpha),
\tag{2}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(F^{-1}(\cdot)\)</span> is the inverse cumulative density function of the demand distribution, and <span class="math notranslate nohighlight">\(\alpha\)</span> is the service level. Unfortunately, the store manager can not directly solve equation (2) since he does not know the true distribution of <span class="math notranslate nohighlight">\(D\)</span>. However, he has collected past demand data that he can use for decision making.</p>
<div class="section" id="Data-and-Pre-processing">
<h3>Data and Pre-processing<a class="headerlink" href="#Data-and-Pre-processing" title="Permalink to this headline">¶</a></h3>
<p>The dataset collected by the restaurant manager includes the demand for seven main ingredients (calamari, fish, shrimp, chicken, koefte, lamb, and steak) over 760 days. In addition it provides a number of demand features including calendar information (day, month, year), weather conditions and more. In the following, we will use this dataset to solve the newsvendor problem when the underlying demand distribution is unknown. Note that for now we will only look at one product, steak. The dataset
is available via <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To load it we can use the <code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from ddop.datasets import load_yaz
yaz = load_yaz(include_prod=[&quot;steak&quot;])
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> will return a dictionary-like object, which contains the following attributes:</p>
<ul class="simple">
<li><p>frame: the whole data frame</p></li>
<li><p>data: the feature matrix</p></li>
<li><p>target: the target variables (in this example the demand targets for steak)</p></li>
<li><p>DESCR: the full description of the dataset.</p></li>
</ul>
<p>To access the data frame, we use the <code class="docutils literal notranslate"><span class="pre">frame</span></code> attribute.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>df = yaz.frame
df
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>weekday</th>
      <th>month</th>
      <th>year</th>
      <th>is_holiday</th>
      <th>weekend</th>
      <th>wind</th>
      <th>clouds</th>
      <th>rainfall</th>
      <th>sunshine</th>
      <th>temperature</th>
      <th>steak</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>FRI</td>
      <td>OCT</td>
      <td>2013</td>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>7.7</td>
      <td>0.1</td>
      <td>150</td>
      <td>15.9</td>
      <td>36</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SAT</td>
      <td>OCT</td>
      <td>2013</td>
      <td>0</td>
      <td>1</td>
      <td>2.7</td>
      <td>6.9</td>
      <td>10.7</td>
      <td>0</td>
      <td>13.2</td>
      <td>30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SUN</td>
      <td>OCT</td>
      <td>2013</td>
      <td>0</td>
      <td>1</td>
      <td>1.4</td>
      <td>8.0</td>
      <td>0.4</td>
      <td>0</td>
      <td>10.6</td>
      <td>16</td>
    </tr>
    <tr>
      <th>3</th>
      <td>MON</td>
      <td>OCT</td>
      <td>2013</td>
      <td>0</td>
      <td>0</td>
      <td>2.3</td>
      <td>6.4</td>
      <td>0.0</td>
      <td>176</td>
      <td>13.3</td>
      <td>22</td>
    </tr>
    <tr>
      <th>4</th>
      <td>TUE</td>
      <td>OCT</td>
      <td>2013</td>
      <td>0</td>
      <td>0</td>
      <td>1.7</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>13.5</td>
      <td>29</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>755</th>
      <td>TUE</td>
      <td>NOV</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>1.6</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>3.5</td>
      <td>32</td>
    </tr>
    <tr>
      <th>756</th>
      <td>WED</td>
      <td>NOV</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>2.2</td>
      <td>0.0</td>
      <td>362</td>
      <td>14.6</td>
      <td>38</td>
    </tr>
    <tr>
      <th>757</th>
      <td>THU</td>
      <td>NOV</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>0.7</td>
      <td>0.0</td>
      <td>405</td>
      <td>14.7</td>
      <td>24</td>
    </tr>
    <tr>
      <th>758</th>
      <td>FRI</td>
      <td>NOV</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>6.9</td>
      <td>0.0</td>
      <td>44</td>
      <td>16.0</td>
      <td>32</td>
    </tr>
    <tr>
      <th>759</th>
      <td>SAT</td>
      <td>NOV</td>
      <td>2015</td>
      <td>0</td>
      <td>1</td>
      <td>1.9</td>
      <td>5.6</td>
      <td>0.0</td>
      <td>46</td>
      <td>17.3</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
<p>760 rows × 11 columns</p>
</div></div>
</div>
<p>Note: For more detailed information about individual columns, we can print the full description of the dataset using the ‘DESCR’ attribute. Similarly, we can access the feature data using the <code class="docutils literal notranslate"><span class="pre">data</span></code> attribute and the target variables with the <code class="docutils literal notranslate"><span class="pre">target</span></code> attribute.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># the feature matrix
X = yaz.data

# the target variables
y = yaz.target
</pre></div>
</div>
</div>
<p>As we want to compare different models in the course of this tutorial, we have to split the data into training and test set. While we use the training set to build a model, we need the test set to evaluate it on unknown data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=False)
</pre></div>
</div>
</div>
<p>Note that some models that we are going to use cannot handle categorical features. For this reason, we also load a one-hot encoded version of feature matrix <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_encoded = load_yaz(include_prod=[&quot;steak&quot;], one_hot_encoding=True).data
X_train_encoded, X_test_encoded = train_test_split(X_encoded, train_size=0.75, shuffle=False)
</pre></div>
</div>
</div>
<p>In addition, we normalize the feature matrix, since some models may be sensitive to variance in the data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train_encoded)
X_train_scaled = scaler.transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Parametric-Approach">
<h2>Parametric Approach<a class="headerlink" href="#Parametric-Approach" title="Permalink to this headline">¶</a></h2>
<p>Let us now use the Yaz dataset to determine the optimal inventory quantity of steak for a given day. One way we can use the data is to estimate the true demand distribution based on the past demand samples. To do this, we first explore the data with a simple histogram. For plotting we use <code class="docutils literal notranslate"><span class="pre">seaborn</span></code> - a Python data visualization library.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># plot a histogram
sns.distplot(y_train, hist=True, norm_hist=False, kde=False,
             hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Frequency&#39;)
plt.xlabel(&#39;Demand&#39;)
plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_tutorial_24_0.png" src="../_images/tutorial_modules_tutorial_24_0.png" />
</div>
</div>
<p>As we can see, it looks like the demand follows a normal distribution. We therefore estimate mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> to fit a normal distribution to the data</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine mean and standard deviation
mean = round(y_train.mean(),2)
std = round(y_train.std(),2)

# plot histogram and fit a normal distribution to it
sns.distplot(y_train, hist=True, norm_hist=False, fit=norm,
             kde=False, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Empirical probability&#39;)
plt.xlabel(&#39;Demand&#39;)
plt.title(&quot;Normal distribution with mean = %.2f and std = %.2f&quot; % (mean, std))

plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_tutorial_26_0.png" src="../_images/tutorial_modules_tutorial_26_0.png" />
</div>
</div>
<p>In the next step, we can use this distribution to determine how many steaks to defrost overnight. But before that, we have to define the under- and overage costs for steak. The store manager tells us that each unit of unsold steak costs 5€ and each unit of demand that cannot be met costs 15€.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cu = 15
co = 5
</pre></div>
</div>
</div>
<p>Finally, we can implement equation (2), which then tells us how many steaks to defrost for a single day (the optimal inventory quantity <span class="math notranslate nohighlight">\(q\)</span>).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine optimal inventory quantity
q = norm(mean,std).ppf(cu/(cu+co))[0]
round(q)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
30.0
</pre></div></div>
</div>
<p>We call this the traditional parametric approach, we first assume the demand falls in a family of parametric distributions, estimate its parameters based on past demand samples, and then solve the initial optimization problem (2).</p>
<p>To get a representation of the decision quality, we calculate the average costs over all days in the test set. For this, we use the <code class="docutils literal notranslate"><span class="pre">average_costs</span></code> function implemented in <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. The function takes four arguments: the true values, the predicted values, the underage costs, and the overage costs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># create array with length equal to number of observations in test set filled with the value of the inventory quantity q
y_pred = np.full(y_test.shape[0],q)

# calculate average costs
average_costs(y_true=y_test, y_pred=y_pred, cu=cu, co=co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
66.46472249886655
</pre></div></div>
</div>
<p>As you can see, we have average costs of 66.46€ associated with our decision. But can we do better? To answer this, let us go back to our data. So far we only used past steak demand samples. However, we have access to exogenous features that may have predictive power for demand, e.g. the weekday. So next, we investigate whether the day of the week has an impact on demand. To do this, we determine mean and standard deviation for each weekday.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># group demand by weekday
data = pd.concat([X_train, y_train], axis=1)
data_by_weekday = data.groupby(&quot;weekday&quot;)[&quot;steak&quot;]

# determine mean and standard deviation for each weekday
mean_by_weekday = data_by_weekday.mean()
std_by_weekday = data_by_weekday.std()
d = {&#39;Mean&#39;:mean_by_weekday, &#39;Std&#39;:std_by_weekday}
mean_and_std_by_weekday = pd.DataFrame(d).round(2)
mean_and_std_by_weekday
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Mean</th>
      <th>Std</th>
    </tr>
    <tr>
      <th>weekday</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FRI</th>
      <td>26.43</td>
      <td>8.06</td>
    </tr>
    <tr>
      <th>MON</th>
      <td>18.79</td>
      <td>7.44</td>
    </tr>
    <tr>
      <th>SAT</th>
      <td>37.80</td>
      <td>12.21</td>
    </tr>
    <tr>
      <th>SUN</th>
      <td>16.55</td>
      <td>6.08</td>
    </tr>
    <tr>
      <th>THU</th>
      <td>21.89</td>
      <td>6.79</td>
    </tr>
    <tr>
      <th>TUE</th>
      <td>20.27</td>
      <td>6.33</td>
    </tr>
    <tr>
      <th>WED</th>
      <td>21.80</td>
      <td>6.54</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We can clearly see that the demand varies greatly depending on the day of the week. We can clearly see that demand varies greatly depending on the day of the week. While the highest amount of steak is sold on Saturdays, the least amount is sold on Sundays and Mondays. One way to take this information into account is to fit a normal distribution for the samples of each weekday, respectively. As before, we can then use these distributions to determine the optimal inventory quantity by solving
equation (2).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine decision q depending on the weekday
days = [&#39;MON&#39;,&#39;TUE&#39;,&#39;WED&#39;,&#39;THU&#39;,&#39;FRI&#39;,&#39;SAT&#39;,&#39;SUN&#39;]
q_by_weekday = {}
for day in days:
  mean = mean_and_std_by_weekday.loc[day][&#39;Mean&#39;]
  std = mean_and_std_by_weekday.loc[day][&#39;Std&#39;]
  q_by_weekday[day] = round(norm(mean, std).ppf(cu/(cu+co)))
q_by_weekday
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;FRI&#39;: 32.0,
 &#39;MON&#39;: 24.0,
 &#39;SAT&#39;: 46.0,
 &#39;SUN&#39;: 21.0,
 &#39;THU&#39;: 26.0,
 &#39;TUE&#39;: 25.0,
 &#39;WED&#39;: 26.0}
</pre></div></div>
</div>
<p>Given the optimal decision for each weekday, we can write a simple predict function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  Get the decision for each sample in X depending on the weekday.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values.
  &quot;&quot;&quot;

  pred = []
  for index, sample in X.iterrows():
    day = sample[&#39;weekday&#39;]
    pred.append(q_by_weekday[day])

  return np.array(pred)
</pre></div>
</div>
</div>
<p>We apply the function to the test set and calculate the average costs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict optimal inventory quantity
y_pred = predict(X_test)

# calculate average costs
average_costs(y_test, y_pred, cu, co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
54.078947368421055
</pre></div></div>
</div>
<p>Now look, we were able to reduce the average costs from 66.46€ to 54.08€. That is a great improvement! But maybe we can do even better by considering the other features of the dataset as well. To do this, we could group observations with same feature values and then estimate a normal distribution for each group - like we did before but now with more features than just the weekday. However, this has two main drawbacks: 1. Imagine we have two features, the weekday and the month. If we were to
estimate a distribution for each feature combination, we would have to fit a total of 7*12=84 distributions. That sounds like a lot of work, doesn’t it? 2. To stay with the example: We consider the case where it is Monday and January. As you can see below, we only have 8 samples with the same feature values. Such a small number of samples makes it hard to fit a distribution. Moreover, with an increasing number of features we may not have a single sample with the same feature values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train[(X_train[&#39;weekday&#39;]==&#39;MON&#39;)&amp;(X_train[&#39;month&#39;]==&quot;JAN&quot;)].shape[0]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
8
</pre></div></div>
</div>
<p>Instead of estimating a distribution for each group of samples with the same feature values, we can train a machine learning model to predict the demand. For example, we can use the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. Note that we set the maximum depth of the tree to 10 to avoid overfitting.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict demand using a regression tree
from sklearn.tree import DecisionTreeRegressor
DT = DecisionTreeRegressor(max_depth=10,random_state=1)
DT.fit(X_train_scaled,y_train)
pred = DT.predict(X_test_scaled)
</pre></div>
</div>
</div>
<p>Of course we cannot assume the model to be perfect: first, because of the model error itself, and second, because of the uncertainty in demand. For this reason, we need to adjust the predictions for uncertainty to obtain optimal decisions. We can get a representation of the remaining uncertainty by estimating the distribution of the prediction error on the training set. Assuming the error to be normally distributed, we determine the parameters <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine mean and standard deviation of the prediction error
error = y_train[&quot;steak&quot;]-DT.predict(X_train_scaled)
error_mean = error.mean()
error_std = error.std()
</pre></div>
</div>
</div>
<p>Next, we pass the error distribution to equation (2) to determine an additional safety buffer that adjusts the predictions based on the forecast error by balancing the expected overage and underage costs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine safety buffer
safety_buffer = norm(error_mean, error_std).ppf(cu/(cu+co))
</pre></div>
</div>
</div>
<p>The final order decision is then the sum of both the prediction generated by our model and the safety buffer. More formally, the solution to the newsvendor problem can then be stated as:</p>
<p><span class="math">\begin{equation}
q(x)^{*} = \mu(x)+\Phi^{-1}(\alpha),
\tag{3}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mu(\cdot)\)</span> is the function of the machine-learning model that predicts the demand given feature vector <span class="math notranslate nohighlight">\(x\)</span>, and is <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> the inverse cdf of the error distribution with parameter <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>. Consequently, in the next step, we add the safety buffer to the predictions to obtain optimal inventory quantities.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># add safety buffer to the predictions of the model
pred = pred + safety_buffer
</pre></div>
</div>
</div>
<p>Finally, we calculate the corresponding average costs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># calculate average costs
average_costs(y_test, pred, cu, co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
51.668449152885316
</pre></div></div>
</div>
<p>Again, we were able to reduce our average costs from 54.08€ to 51.67€.</p>
<div class="section" id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h3>
<p>Let us now summarize what we have learned so far:</p>
<ul class="simple">
<li><p>In case we do not know the demand distribution, we can estimate the distribution based on past demand samples and then solve equation (2) to determine the optimal inventory quantity.</p></li>
<li><p>We can improve the decision by taking features into account. To do this, we group the data based on their feature values and estimate a distribution for each group. However, this does not work well for a large number of features.</p></li>
<li><p>Instead, we can use a machine-learning model to predict demand. Since we cannot assume the model to be perfect, we estimate the distribution of the prediction error to calculate an additional safety buffer. The optimal decision is then the sum of prediction and safety buffer.</p></li>
</ul>
</div>
</div>
<div class="section" id="Data-Driven-Approaches">
<h2>Data-Driven Approaches<a class="headerlink" href="#Data-Driven-Approaches" title="Permalink to this headline">¶</a></h2>
<p>So far we have only considered the traditional parametric way to solve the newsvendor problem. In this part of the tutorial, we introduce different <strong>“data-driven”</strong> approaches to solve the newsvendor problem. In contrast to the traditional way of first estimating the demand distribution and then solving the initial optimization problem, these approaches directly prescribe decisions from data.</p>
<div class="section" id="Sample-Average-Approximation">
<h3>Sample Average Approximation<a class="headerlink" href="#Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>The simplest <strong>data-driven</strong> approach to solve the newsvendor problem is called <strong>Sample Average Approximation (SAA)</strong>. The idea behind <strong>SAA</strong> is to determine the optimal inventory quantity by finding the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average cost on past demand samples. Formally, the optimization problem can be stated as follows:</p>
<p><span class="math">\begin{equation}q^{*}=\min _{q \geq 0} \frac{1}{n} \sum_{i=1}^{n}\left[c u\left(d_{i}-q\right)^{+}+c o\left(q-d_{i}\right)^{+}\right]
\tag{4}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the total number of samples and <span class="math notranslate nohighlight">\(d_i\)</span> is the i-th demand observation. Moreover, <span class="math notranslate nohighlight">\((\cdot)^+\)</span> is a function that returns <span class="math notranslate nohighlight">\(0\)</span> if its argument is negative, and else its argument. Using this function we ensure that we multiply missing units by the underage costs and excess units by the overage costs. To fully understand how <strong>SAA</strong> works, we go through a simple example.</p>
<p><strong>Example:</strong> Determine how many steaks to defrost by applying <strong>SAA</strong>. The past demand for steak is given by: <span class="math notranslate nohighlight">\(D=[27,29,30]\)</span>. You can sell steak in your restaurant for 15€ (underage costs), while unsold units incur overage costs of 5€.</p>
<p>In the following, we try to minimize the optimization problem (4). For simplicity, we do not write down both terms <span class="math notranslate nohighlight">\(cu(d_i-q)^+\)</span> and <span class="math notranslate nohighlight">\(co(q-d_i)^+\)</span>. One of them is always zero because we can have either underage units or overage units.</p>
<p><span class="math">\begin{equation}
.....\\
q=27: \frac{1}{3}\Bigl[15*(27-27)+15*(29-27)+15*(30-27)\Bigl]=25\\
q=28: \frac{1}{3}\Bigl[5*(28-27)+15*(29-28)+15*(30-28)\Bigl]=16,67\\
q=29: \frac{1}{3}\Bigl[5*(29-27)+15*(29-29)+15*(30-29)\Bigl]=8,33\\
q=30: \frac{1}{3}\Bigl[5*(30-27)+5*(30-29)+15*(30-30)\Bigl]=6,67\\
q=31: \frac{1}{3}\Bigl[5*(31-27)+5*(31-29)+5*(31-30)\Bigl]=11,67\\
.....
\end{equation}</span></p>
<p>Based on our calculation, the optimal decision is given by <span class="math notranslate nohighlight">\(q=30\)</span>.</p>
<p>Now that we know how <strong>SAA</strong> works, we can apply the approach on the Yaz dataset by using the class <code class="docutils literal notranslate"><span class="pre">SampleAverageApproximationNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the model we pass the under- and overage costs to the constructor. Subsequently, we fit the model to the past demand samples and determine the optimal decision using the predict method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>SAA = SampleAverageApproximationNewsvendor(cu,co)
SAA.fit(y_train)
SAA.predict()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[28]])
</pre></div></div>
</div>
<p>Based on <strong>SAA</strong> the optimal decision is 28. To get a representation of the model performance, we use the model’s <code class="docutils literal notranslate"><span class="pre">score</span></code> function. Just like the <code class="docutils literal notranslate"><span class="pre">average_costs</span></code> function, the <code class="docutils literal notranslate"><span class="pre">score</span></code> method calculates the average cost. The advantage is that we can directly calculate the costs without having to predict all training samples first.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>-SAA.score(y_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
60.23684210526316
</pre></div></div>
</div>
<p>With average cost of 60.24€, <strong>SAA</strong> performs better compared to the traditional parametric approach without features (66.46€). In contrast, <strong>SAA</strong> performs worse compared to the parametric approach with features (51.67€). However, this is not really surprising since we already know that exogenous features can improve decision making.</p>
</div>
<div class="section" id="Weighted-Sample-Average-Approximation">
<h3>Weighted Sample Average Approximation<a class="headerlink" href="#Weighted-Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>Even though <strong>SAA</strong> is a common and effective approach, it only considers past demand samples. However, in the first part of the tutorial we have seen that we can improve our decision by taking into account features such as the weekday. We now want to do the same here by determining the optimal <strong>SAA</strong> decision for each weekday separately.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine q depending on the weekday using SAA
q_by_weekday = {}

SAA = SampleAverageApproximationNewsvendor(cu,co)

for day in days:
  demand = data_by_weekday.get_group(day)
  SAA.fit(demand)
  q_by_weekday[day] = SAA.predict().item(0)

print(q_by_weekday)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;MON&#39;: 21, &#39;TUE&#39;: 24, &#39;WED&#39;: 26, &#39;THU&#39;: 26, &#39;FRI&#39;: 31, &#39;SAT&#39;: 45, &#39;SUN&#39;: 20}
</pre></div></div>
</div>
<p>Given the optimal decision for each day, we can write a new prediction function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  Get the prediction for each sample in X depending on the weekday.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values.
  &quot;&quot;&quot;

  pred = []
  for index, sample in X.iterrows():
    day = sample[&#39;weekday&#39;]
    pred.append(q_by_weekday[day])

  return np.array(pred)
</pre></div>
</div>
</div>
<p>We can then apply the function on the test set and calculate the average costs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pred = predict(X_test)
average_costs(y_test,pred,cu,co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
52.23684210526316
</pre></div></div>
</div>
<p>Again, we were able to reduce the average costs from 60.24€ to 52.24€ by taking into account the weekday. We can now go a step further by considering more features. To do this, we could use a separate model for the samples with the same feature values. However, as we have seen in the traditional parametric approach, this can become problematic when we have only a small number of samples with the same feature values. Another way to take feature information into account is to determine a weight
for each sample based on the similarity to a new instance and optimize <strong>SAA</strong> against a re-weighting of the data. To determine the sample weights, we can use different machine learning techniques, such as a regression tree. For a better understanding, consider the following example.</p>
<p><strong>Example:</strong> Say we have the following past demand samples with feature vector <span class="math notranslate nohighlight">\(x_i=(Weekend, Temperature)\)</span> and we want to calculate their weights based on a new sample <span class="math notranslate nohighlight">\(x=(0,18)\)</span>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 50%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sample</p></th>
<th class="head"><p>Weekend</p></th>
<th class="head"><p>Temperature [in °C]</p></th>
<th class="head"><p>Demand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>19</p></td>
<td><p>27</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>25</p></td>
<td><p>29</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>23</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>25</p></td>
<td><p>18</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>0</p></td>
<td><p>24</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>22</p></td>
<td><p>23</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>0</p></td>
<td><p>11</p></td>
<td><p>21</p></td>
</tr>
</tbody>
</table>
<p>First, we train a regression tree to predict the demand given the features weekday and temperature. The resulting tree looks like this:</p>
<p><img alt="cae2f23606544c839c74c790fb0db673" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=1fXW6PV2bUcnZSNByTde39k8hEgRB_5aV" style="width: 450px;" /></p>
<p>The regression tree splits our data into four leafs based on the two features. To determine the sample weights, we look to which leaf the new instance <span class="math notranslate nohighlight">\(x=(0,18)\)</span> belongs. In the first level of the tree we follow the left path since it is no weekend. Then, given the temperature forecast of <span class="math notranslate nohighlight">\(18°C\)</span>, we end up in leaf 1 together with sample 1, 6, 7. This means that these observations are most similar to <span class="math notranslate nohighlight">\(x\)</span>. Using this information, we assign a weight of <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> to each
of the three samples falling into the same leaf. The sample weights are then given by:</p>
<p><span class="math">\begin{equation}
w_1=\frac{1}{3},\:w_2=0,\:w_3=0,\:w_4=0,\:w_5=0,\:w_6=\frac{1}{3},\:w_7=\frac{1}{3}
\end{equation}</span></p>
<p>Now that we have calculated the sample weights, we can solve the optimization problem like before. The only difference is that we multiply each observation with its corresponding weight, which indicates the similarity to the new sample.</p>
<p><span class="math">\begin{equation}
q=18: \frac{1}{3}\bigl[15*(27-18)\bigl]+...+\frac{1}{3}\bigl[15*(23-18)\bigl]+\frac{1}{3}\bigl[15*(21-18)\bigl] = 85\\
......\\
q=26: \frac{1}{3}\bigl[15*(27-26)\bigl]+...+\frac{1}{3}\bigl[5*(26-23)\bigl]+\frac{1}{3}\bigl[5*(26-21)\bigl] = 18.33\\
q=27: \frac{1}{3}\bigl[15*(27-27)\bigl]+...+\frac{1}{3}\bigl[5*(27-23)\bigl]+\frac{1}{3}\bigl[5*(27-21)\bigl] = 16.67\\
q=28: \frac{1}{3}\bigl[5*(28-27)\bigl]+...+\frac{1}{3}\bigl[5*(28-23)\bigl]+\frac{1}{3}\bigl[5*(28-21)\bigl] = 21.67\\
......\\
q=30: \frac{1}{3}\bigl[5*(30-27)\bigl]+...+\frac{1}{3}\bigl[5*(30-23)\bigl]+\frac{1}{3}\bigl[5*(30-21)\bigl] = 30.67
\end{equation}</span></p>
<p>Based on our calculation, <span class="math notranslate nohighlight">\(q=27\)</span> is the quantity minimizing the average costs on the weighted samples.</p>
<p>We call this approach <strong>“weigthed Sample Average Approximation (wSAA)”</strong> as it can be seen as weighted form of <strong>SAA</strong>. More formally the problem can be stated as follows:</p>
<p><span class="math">\begin{equation}
q(x)^*=\min_{q\geq 0} \sum_{i=1}^{n}w_i(x)\bigl[cu(d_i-q)^+ + co(q-d_i)\bigl],
\tag{5}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the feature vector of a new instance and <span class="math notranslate nohighlight">\(w_i(\cdot)\)</span> is a function that assigns a weight <span class="math notranslate nohighlight">\(w_i\in [0,1]\)</span> to each past demand sample <span class="math notranslate nohighlight">\(S_i(x_i,d_i)\)</span> based on the similarity of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x_i\)</span>. In the example above we used a weight function based on a decision tree (DT), given by:</p>
<p><span class="math">\begin{equation}
w_{i}^{Tree}(x)=\frac{\mathbb{1}x_i \in R(x)}{N(x)},
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}\)</span> is the indicator function, <span class="math notranslate nohighlight">\(R(x)\)</span> is the leaf containing <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(N(x)\)</span> is the number of samples falling into leaf <span class="math notranslate nohighlight">\(R(x)\)</span>. In other words, the function will give a weight of <span class="math notranslate nohighlight">\(\frac{1}{N(x)}\)</span> to sample <span class="math notranslate nohighlight">\(i\)</span> if <span class="math notranslate nohighlight">\(x_i\)</span> belongs to the same leaf as the new instance <span class="math notranslate nohighlight">\(x\)</span>, and otherwise zero.</p>
<p>Instead of using a DT, we can also use other machine learning methods to determine the sample weights, for example k-nearest-neighbors (kNN):</p>
<p><span class="math">\begin{equation}
w_{i}^{k \mathrm{NN}}(x)=\frac{1}{k} \mathbb{1}\left[x_i \text { is a } k \mathrm{NN} \text { of } x\right],
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of neighbors. In simple terms, the function will give a weight of <span class="math notranslate nohighlight">\(\frac{1}{k}\)</span> to sample <span class="math notranslate nohighlight">\(x_i\)</span> if it is a k-nearest-neighbor of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We can apply these approaches on our dataset by using the class <code class="docutils literal notranslate"><span class="pre">RandomForestWeightedNewsvendor</span></code> and <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the models, we pass the overage- and underage costs to the constructor. Moreover, we define some addition model-specific parameters. For instance, we set the maximum depth for <code class="docutils literal notranslate"><span class="pre">DecisionTreeWeightedNewsvendor</span></code> to <span class="math notranslate nohighlight">\(8\)</span> and the number of neighbors for <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> to <span class="math notranslate nohighlight">\(30\)</span>. After we have initialized our
models we fit them on the training data and calculate the average costs on the test set by using the <code class="docutils literal notranslate"><span class="pre">score</span></code> function:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DTW = DecisionTreeWeightedNewsvendor(cu=cu, co=co, max_depth=8,random_state=1)
DTW.fit(X_train_scaled, y_train[&quot;steak&quot;])
DTW_score = DTW.score(X_test_scaled,y_test)
print(&quot;Avg. cost DTW: &quot;,-DTW_score)

kNNW = KNeighborsWeightedNewsvendor(cu=cu, co=co, n_neighbors=30)
kNNW.fit(X_train_scaled, y_train)
kNNW_score = kNNW.score(X_test_scaled,y_test)
print(&quot;Avg. cost kNNW: &quot;, -kNNW_score)

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost DTW:  47.6578947368421
Avg. cost kNNW:  47.473684210526315
</pre></div></div>
</div>
<p>These are the best results so far. Both models, <strong>DTW</strong> (47.66€) and <strong>kNNW</strong> (47.47€), outperform all other models we have seen so far.</p>
<hr class="docutils" />
</div>
<div class="section" id="Empirical-Risk-Minimization">
<h3>Empirical Risk Minimization<a class="headerlink" href="#Empirical-Risk-Minimization" title="Permalink to this headline">¶</a></h3>
<p>With <strong>wSAA</strong> we have already learned a data-driven approach that is able take exogenous features into account. However, to obtain a decision <span class="math notranslate nohighlight">\(q\)</span>, we have to first determine weights and then solve an optimization problem for every new sample <span class="math notranslate nohighlight">\(x\)</span>. But wouldn’t it be great to learn a function that instead maps directly from features <span class="math notranslate nohighlight">\(x\in X\)</span> to a decision <span class="math notranslate nohighlight">\(q\in Q\)</span>? A natural way to obtain such a function is though <a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">empirical risk
minimization</a>: <span class="math">\begin{equation}
\min_{q(\cdot)\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-q(x_i))^+ + co(q(x_i)-d_i)^+\bigl],
\tag{6}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is the function that maps from feature space <span class="math notranslate nohighlight">\(X\)</span> to decision space <span class="math notranslate nohighlight">\(Q\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is its function class. Again, <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the i-th sample, and <span class="math notranslate nohighlight">\(d_i\)</span> is the corresponding demand value. To say it simple, we try to find the function <span class="math notranslate nohighlight">\(q(\cdot):X\rightarrow Q\)</span> that minimizes the average costs on <span class="math notranslate nohighlight">\(n\)</span> past demand samples (the empirical risk). The most straightforward way to do this is based on linear
regression.</p>
<div class="section" id="Linear-Regression">
<h4>Linear Regression<a class="headerlink" href="#Linear-Regression" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have only a single feature, say temperature, and given the value for the temperature we want to know how many steaks to defrost. In other words, we want to find a function <span class="math notranslate nohighlight">\(q(temp)\)</span> that maps from temperature to a decision <span class="math notranslate nohighlight">\(q\)</span>. Of course, the first thing we need to do is to collect some data.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 61%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sample</p></th>
<th class="head"><p>Temperature [in °C]</p></th>
<th class="head"><p>Demand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>5</p></td>
<td><p>16</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>7</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>10</p></td>
<td><p>14</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>12</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>15</p></td>
<td><p>11</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>18</p></td>
<td><p>7</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>20</p></td>
<td><p>9</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>21</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>23</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>25</p></td>
<td><p>4</p></td>
</tr>
</tbody>
</table>
<p>We plot the data and it looks like there is a linear relationship between temperature and demand (see figure below). Consequently, to solve our problem the first thing that comes into mind is linear regression. For this reason, we construct a linear function <span class="math notranslate nohighlight">\(q(temp)=b+w*temp\)</span>, where <span class="math notranslate nohighlight">\(b\)</span> is the intercept term, and <span class="math notranslate nohighlight">\(w\)</span> is a weight. We can think of the intercept as a base demand to which we add or subtract a certain amount depending on the temperature and weight <span class="math notranslate nohighlight">\(w\)</span>. To
find the optimal mapping from temperature to demand, we now have to learn the best fitting values for <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span>. We start by setting <span class="math notranslate nohighlight">\(b=16\)</span> and <span class="math notranslate nohighlight">\(w=-0,4\)</span>, which gives us function <span class="math notranslate nohighlight">\(q(temp)=16-0,4*temp\)</span> that already fits the data quite well. Still, for each sample <span class="math notranslate nohighlight">\(i\)</span> we obtain an error representing the difference between the actual demand <span class="math notranslate nohighlight">\(d_i\)</span> and the estimated decision of <span class="math notranslate nohighlight">\(q(temp_i)\)</span>. In the figure below these errors are illustrated as dotted
lines.</p>
<p><img alt="a235558608474778b135839d709ded30" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=1IbPX_WRmSURbEypcyYdwwcvSo7aUjfDZ" style="width: 600px;" /></p>
<p>According to linear regression, the goal would be to adjust the values for <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> in a way that minimizes the mean difference (error) between the actual demand and the estimated decision. Formally speaking:</p>
<p><span class="math">\begin{equation}
\min_{b,w} \frac{1}{n}\sum_{i=1}^{n}\Bigl(d_i-(b+w*temp)\Bigl)^2
\end{equation}</span></p>
<p>In our case, however, this can be problematic because overage would have the same effect as underage. Now recall that each unit of unsold steak costs 5€, while each unit of demand that cannot be satisfied costs 15€. In other words, one unit too little costs three times more than one unit too much. Consequently, we minimize the average cost rather than the mean difference. The resulting optimization problem is then given by:</p>
<p><span class="math">\begin{equation}
\min_{b,w} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-(b+w*temp))^+ + co((b+w*temp)-d_i)^+\bigl]
\end{equation}</span></p>
<p>Doesn’t this look like equation <span class="math notranslate nohighlight">\((6)\)</span>, which we introduced at the very beginning of this chapter? The only difference is that we have defined <span class="math notranslate nohighlight">\(q(\cdot)\)</span> as a linear decision function of the form <span class="math notranslate nohighlight">\(q(temp)=b+w*temp\)</span>. To solve this optimization problem, we can use the class <code class="docutils literal notranslate"><span class="pre">LinearRegressionNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>temp = [[5],[7],[10],[12],[15],[18],[20],[21],[23],[25]]
demand = [16,12,14,10,11,7,9,6,8,4]
mdl = LinearRegressionNewsvendor(cu=15,co=5)
mdl.fit(X=temp,y=demand)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LinearRegressionNewsvendor(co=5, cu=15)
</pre></div></div>
</div>
<p>After fitting the model, we can access the intercept term <span class="math notranslate nohighlight">\(b\)</span> via the argument <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> and the weight <span class="math notranslate nohighlight">\(w\)</span> via the argument <code class="docutils literal notranslate"><span class="pre">feature_weights_</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;b:&quot;,mdl.intercept_)
print(&quot;w:&quot;,mdl.feature_weights_)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
b: [18.333333]
w: [[-0.46666667]]
</pre></div></div>
</div>
<p>Thus, the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that fits our data best is given by:</p>
<p><span class="math">\begin{equation}
q(temp)=18.3-0.47*temp
\end{equation}</span></p>
<p>In the next step we want to know how many steaks to defrost for tomorrow. We check the weather forecast, which tells us that it will be 25 degrees. Given the temperature, the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> then tells us that the optimal inventory stock is: <span class="math notranslate nohighlight">\(18.3-0.47*10=13.6\)</span>. Instead of determining the decision ourselves, we can also use the model’s ´predict` method:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mdl.predict([[10]])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[13.6666663]])
</pre></div></div>
</div>
<p>Of course, this was just a simple example to illustrate the concept of this approach. As we know, the Yaz dataset provides a lot more features than just the temperature. Consequently, we are looking for a function of the form:</p>
<p><span class="math">\begin{equation}
q(x)=b+w_1*x_1+...+w_m*x_m=b+\sum_{j=1}^{m}w_j*x_j,
\tag{7}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(x_j\)</span> represents the value of feature <span class="math notranslate nohighlight">\(j\)</span>-th from sample <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(w_j\)</span> is the corresponding feature weight. As a result, the optimization problem that we want to solve becomes:</p>
<p><span class="math">\begin{equation}
\min_{b,w_1,..,w_m} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-b-\sum_{j=1}^{m}w_j*x_{i,j})^+ + co(b+\sum_{j=1}^{m}w_j*x_{i,j}-d_i)^+\bigl],
\tag{8}
\end{equation}</span></p>
<p>We now have to learn the value for intercept term <span class="math notranslate nohighlight">\(b\)</span> and feature weights <span class="math notranslate nohighlight">\(w_1,...,w_m\)</span>. For now, we do this just for the temperature and the weekdays. Note that we use the one hot encoded version of the data, thus each weekday is represented by a binary column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cols = [&#39;temperature&#39;, &#39;weekday_MON&#39;, &#39;weekday_TUE&#39;, &#39;weekday_WED&#39;, &#39;weekday_THU&#39;, &#39;weekday_FRI&#39;, &#39;weekday_SAT&#39;, &#39;weekday_SUN&#39;]
LRN = LinearRegressionNewsvendor(cu=15,co=5)
LRN.fit(X_train_encoded[cols],y_train)
print(LRN.intercept_)
print(LRN.feature_weights_)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[24.860215]
[[-0.21505376  0.          2.2580645   3.7311828   2.9032258   8.3978495
  22.225806   -1.5913978 ]]
</pre></div></div>
</div>
<p>We note that the intercept term is 24.86. This is now our base demand to which we add or subtract a certain amount, depending on corresponding feature values. For example, we add 22.23 for a Saturday. In contrast, we subtract 1.59 for a Sunday. The weight for the temperature is -0.215 so we would subtract 2.15 in case it is 10°C. At this point it should be clear how this approach works and how the parameters are to be interpreted. So in the next step, we can now apply the model to our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LRN = LinearRegressionNewsvendor(cu=15,co=5)
LRN.fit(X_train_encoded,y_train)
-LRN.score(X_test_encoded,y_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
48.932357245389476
</pre></div></div>
</div>
<p>With average costs of €48.93 the model performs slightly worse compared to the <strong>wSAA</strong> approaches. However, it outperforms all other models we have seen so far.</p>
</div>
<div class="section" id="Deep-Learning">
<h4>Deep Learning<a class="headerlink" href="#Deep-Learning" title="Permalink to this headline">¶</a></h4>
<p>In the previous section, we assumed that the demand is a linear combination of features. To find the perfect mapping from features <span class="math notranslate nohighlight">\(x\)</span> to decision <span class="math notranslate nohighlight">\(q\)</span> we therefore specified that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> belongs to the class of linear decision functions. However, sometimes there is no linear relationship, so we need a nonlinear function to describe our data. One way to obtain such a function is by using a deep neural network (DNN).</p>
<p><img alt="1979cfb5dda44f15b7004f76d7e1ef9f" src="https://drive.google.com/uc?export=view&amp;id=1rmgdo9urd4Qx5MQrPu4sO9sto8_ogCm1" /></p>
<p>A DNN uses a cascade of many layers to obtain an output given some input data. In general, it can be distinguished between input-, hidden-, and output-layer, each consisting of a number of neurons. In the first layer the number of neurons corresponds to the number of inputs. In other words, each neuron takes the value of a single feature as input, e.g. the temperature or the weekday. The input-layer is followed by a number of hidden-layers, each presented by an arbitrary number of neurons. In
the output-layer the number of neurons corresponds to the number of outputs. In our example, we only have a single neuron that outputs the decision <span class="math notranslate nohighlight">\(q\)</span> conditional on the features temperature and weekday. The individual neurons of a layer are each connected to the neurons of the layer before and behind. In a graph, the neurons can be represented as nodes and their connections as weighted edges. A neuron takes the outputs of the neurons from the previous layer as inputs. Subsequently, it
computes the weighted sum of its inputs and adds a bias to it. Formally speaking:</p>
<p><span class="math">\begin{equation}
bias+\sum_{l=1}^{L}x_l*w_l,
\tag{9}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(x_l\)</span> is the <span class="math notranslate nohighlight">\(l\)</span>-th input of a neuron and <span class="math notranslate nohighlight">\(w_l\)</span> the corresponding weight. Does this look familiar? This is the exactly the decision function <span class="math notranslate nohighlight">\((7)\)</span> that we used in the regression based approach before. The only difference is that we use the term “bias” for the constant value instead of “intercept”. But what does this mean? If we were just to combine a set of linear functions, we would get a single linear function as a result. In other words, there would be no
difference to the previous approach. This is where the activation function comes into play. The activation function is a non-linear function that transforms the computed value of equation (9) and then outputs the final result of a neuron. For example, the Rectified Linear Unit (ReLU) activation function outputs <span class="math notranslate nohighlight">\(0\)</span> if the input value is negative and its input otherwise. Thus, the DNN models a piecewise linear function, which may looks like this:</p>
<p><img alt="a9339638818347ae9606165fe3013d67" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=18YRxB6jYlqV97FaEPXf-IcRuJt1nWkWI" style="width: 350px;" /></p>
<p>The goal of the network is then to find the function that fits the data best. To find such a function in the linear regression based model, we had to determine the optimal values for the feature weights and the intercept term. This is basically the same here. We just have a lot more weights and intercept terms (biases). Since we are trying to obtain cost-optimal decisions, the network tries to determine the unknown parameters in a way that minimizes the average costs on our data. Thus, the
problem can be stated as follows:</p>
<p><span class="math">\begin{equation}
\min_{w,b} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-\theta(x_i;w,b))^+ + co(\theta(x_i;w,b)-d_i)^+\bigl],
\tag{10}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\theta(\cdot)\)</span> represents the function of the network with weights <span class="math notranslate nohighlight">\(w\)</span> and biases <span class="math notranslate nohighlight">\(b\)</span>. Now look, this is again similar to equation <span class="math notranslate nohighlight">\((6)\)</span>. The only difference is that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is represented by a DNN.</p>
<p>To apply this approach on the Yaz dataset we can use the class <code class="docutils literal notranslate"><span class="pre">DeepLearningNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DLN = DeepLearningNewsvendor(cu=15,co=5)
DLN.fit(X_train_scaled,y_train)
-DLN.score(X_test_scaled,y_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
49.17223325528597
</pre></div></div>
</div>
<p>As we can see, the average cost are higher compared to both the linear regression based model and the WSAA models. One reason for this might be that a neural network often needs more data to learn a good decision function.</p>
</div>
</div>
<div class="section" id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>In this part of the tutorial we have seen three different data-driven approaches to solve the newsvendor problem.</p>
<ul class="simple">
<li><p>In the simplest case where we only have past demand observations we can use sample average approximation <strong>(SAA)</strong> to solve the newsvendor problem. The goal of <strong>SAA</strong> is to find the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average costs on past demand samples.</p></li>
<li><p>However, we have seen that additional demand features can improve decision making, since they usually reduce the degree of uncertainty.</p></li>
<li><p>With weighted sample average approximation <strong>(wSAA)</strong> and empirical risk minimization <strong>(ERM)</strong> we got to know two data-driven approaches that can take such features into account by learning a function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps from features <span class="math notranslate nohighlight">\(x\)</span> to a decision <span class="math notranslate nohighlight">\(q\)</span>.</p></li>
<li><p><strong>wSAA</strong>, on the one hand, defines the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> point-wise. It is based on deriving sample weights from features and optimizing <strong>SAA</strong> against a re-weighting of the training data. To determine the weights we can use different weight functions, e.g., based on KNN or DT regression.</p></li>
<li><p>The <strong>ERM</strong> approach, on the other hand, tries to find the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps from features to a decision by minimizing its empirical risk. Therefore, we have to specify the function space to which the decision function belongs. With <code class="docutils literal notranslate"><span class="pre">LinearRegressionNewsvendor</span></code>, and <code class="docutils literal notranslate"><span class="pre">DeepLearningNewsvendor</span></code> we have seen two models that define <span class="math notranslate nohighlight">\(q(\cdot)\)</span> in a different manner. While the former one assumes that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is a linear decision function, the latter one defines
<span class="math notranslate nohighlight">\(q(\cdot)\)</span> as non-linear function by using as a deep neural network.</p></li>
</ul>
</div>
</div>
<div class="section" id="A-Final-Note">
<h2>A Final Note<a class="headerlink" href="#A-Final-Note" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, you learned how to solve the newsvendor problem based on past demand data. For the specific case of Yaz and the data for the product “steak”, you have seen that several data-driven algorithms can outperform the traditional parametric approach. The best method has been shown to be <strong>wSAA</strong> with a weight function based on k-nearest-neighbors regression. However, it is important to note that there is no such thing as “the best model”. Which model performs best can vary from one
task to another. It may depend on the cost parameters, the number of features or the size of the dataset. This is exactly the reason why <code class="docutils literal notranslate"><span class="pre">ddop</span></code> provides easy access to a set of different models that can be used to find the best one for a given problem. As a next step, you can now build up on what you have learned and apply the different algorithms to some other data. For example, you can start by using the data for a product other than “steak”, or even several products at once. Alternatively,
you can use the bakery dataset that is also provided within <code class="docutils literal notranslate"><span class="pre">ddop</span></code>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../tutorial.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Andreas Philippi

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>