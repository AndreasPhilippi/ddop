{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZWwhvr003qh"
   },
   "source": [
    "# The Data Driven Newsvendor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsyklZjLFdha"
   },
   "source": [
    "The newsvendor problem is the classical single period inventory problem that refers to a situation in which a seller (the newsvendor) has to determine the order quantity of perishable goods for the next selling period under uncertainty in demand. The traditional way to solve the problem assumes that the demand distribution is known. In practice, however, we almost never know the true demand distribution. \n",
    "\n",
    "In the following tutorial, you will get to know different approaches to solve the newsvendor problem when the underlying demand distribution is unknown but the decision maker has access to past demand observations. In this context, we consider the decision problem of Yaz, a fast-casual restaurant in Stuttgart. In addition, you will learn how to apply these approaches using our Python library `ddop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shpRhf_AMkJh"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33U5qv-WKTw5"
   },
   "source": [
    "Before jumping into the tutorial, you should know the basics of Python and be familiar with well known libraries like numpy, pandas, and scikit-learn. To execute code, make sure you have an empty Python virtual environment installed on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NCdRffbpKh_C"
   },
   "outputs": [],
   "source": [
    "pip install ddop==0.6.3 seaborn==0.10.1 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOeFO5aar40f",
    "outputId": "2ba5626e-9d54-4f2e-9d7e-191beee9adbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ddop.metrics import average_costs\n",
    "from ddop.newsvendor import SampleAverageApproximationNewsvendor\n",
    "from ddop.newsvendor import DecisionTreeWeightedNewsvendor\n",
    "from ddop.newsvendor import KNeighborsWeightedNewsvendor\n",
    "from ddop.newsvendor import LinearRegressionNewsvendor\n",
    "from ddop.newsvendor import DeepLearningNewsvendor\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVPG-56Snuhx"
   },
   "source": [
    "## The Newsvendor Problem at Yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvQs1CXvns-c"
   },
   "source": [
    "Let us now start by introducing the decision problem at Yaz. Yaz is a fast casual restaurant in Stuttgart providing great oriental cuisine. The main ingredients for the meals, such as steak, lamb, fish, etc., are prepared at a central factory and are deep-frozen to achieve longer shelf lives. Depending on the estimated demand for the next day, the restaurant manager has to decide how much of the ingredients to defrost over night. These defrosted ingredients/meals then have to be sold within the following day. If the defrosted quantity was too low, each unit of demand that cannot be satisfied incurs underage cost of $cu$. On the other hand, if the quantity was too high, unsold ingredients must be disposed of at overage cost of $co$. Therefore, the store manager wants to choose the order quantity that minimizes the sum of the expected costs. \n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1UPRyUC56wMVd554iHsvOks-MBJaXYwMf)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOGx8IYP3A11"
   },
   "source": [
    "More formally, the problem that the store manager is trying to solve is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{q\\geq 0} = E_D[cu(D-q)^+ + co(q-D)^+],\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ is the uncertain demand, $q$ is the order quantity, $cu$ and $co$ are the per-unit under and overage costs, and $(\\cdot)^+ := \\max\\{0,\\cdot\\}$ is a function that returns 0 if its argument is negative, and else its argument. The optimization problem at hand is what is known as the newsvendor problem. If the demand distribution is known, then the optimal decision can be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "q^*=F^{-1}\\biggl(\\frac{cu}{cu+co}\\biggl)=F^{-1}(\\alpha),\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $F^{-1}(\\cdot)$ is the inverse cumulative density function of the demand distribution, and $\\alpha$ is the service level. Unfortunately, the store manager can not directly solve equation (2) since he does not know the true distribution of $D$. However, he has collected past demand data that he can use for decision making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2MG2OPDjaGS"
   },
   "source": [
    "### Data and Pre-processing\n",
    "The dataset collected by the restaurant manager includes the demand for seven main ingredients (calamari, fish, shrimp, chicken, koefte, lamb, and steak) over 760 days. In addition it provides a number of demand features including calendar information (day, month, year), weather conditions and more. In the following, we will use this dataset to solve the newsvendor problem when the underlying demand distribution is unknown. Note that for now we will only look at one product, steak. The dataset is available via `ddop`. To load it we can use the `load_yaz` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q9mlUIGDm9CO"
   },
   "outputs": [],
   "source": [
    "from ddop.datasets import load_yaz\r\n",
    "yaz = load_yaz(include_prod=[\"steak\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU9nUd_qBE0"
   },
   "source": [
    "`load_yaz` will return a dictionary-like object, which contains the following attributes:\r\n",
    "\r\n",
    "- frame: the whole data frame\r\n",
    "- data: the feature matrix\r\n",
    "- target: the target variables (in this example the demand targets for steak)\r\n",
    "- DESCR: the full description of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTyuQ7UF0emn"
   },
   "source": [
    "To access the data frame, we use the `frame` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "UL-ZdhdltKwC",
    "outputId": "a2bce733-17e5-4bd8-c1c3-5f9709d2398b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>wind</th>\n",
       "      <th>clouds</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>temperature</th>\n",
       "      <th>steak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRI</td>\n",
       "      <td>OCT</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>150</td>\n",
       "      <td>15.9</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAT</td>\n",
       "      <td>OCT</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>6.9</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUN</td>\n",
       "      <td>OCT</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MON</td>\n",
       "      <td>OCT</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176</td>\n",
       "      <td>13.3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TUE</td>\n",
       "      <td>OCT</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>TUE</td>\n",
       "      <td>NOV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>WED</td>\n",
       "      <td>NOV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>362</td>\n",
       "      <td>14.6</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>THU</td>\n",
       "      <td>NOV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405</td>\n",
       "      <td>14.7</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>FRI</td>\n",
       "      <td>NOV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>SAT</td>\n",
       "      <td>NOV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>17.3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    weekday month  year  is_holiday  ...  rainfall  sunshine  temperature  steak\n",
       "0       FRI   OCT  2013           0  ...       0.1       150         15.9     36\n",
       "1       SAT   OCT  2013           0  ...      10.7         0         13.2     30\n",
       "2       SUN   OCT  2013           0  ...       0.4         0         10.6     16\n",
       "3       MON   OCT  2013           0  ...       0.0       176         13.3     22\n",
       "4       TUE   OCT  2013           0  ...       0.0         0         13.5     29\n",
       "..      ...   ...   ...         ...  ...       ...       ...          ...    ...\n",
       "755     TUE   NOV  2015           0  ...       0.0         0          3.5     32\n",
       "756     WED   NOV  2015           0  ...       0.0       362         14.6     38\n",
       "757     THU   NOV  2015           0  ...       0.0       405         14.7     24\n",
       "758     FRI   NOV  2015           0  ...       0.0        44         16.0     32\n",
       "759     SAT   NOV  2015           0  ...       0.0        46         17.3     20\n",
       "\n",
       "[760 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = yaz.frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Beu_enBAI1yX"
   },
   "source": [
    "Note: For more detailed information about individual columns, we can print the full description of the dataset using the 'DESCR' attribute. Similarly, we can access the feature data using the `data` attribute and the target variables with the `target` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZsWcOIzbKXBW"
   },
   "outputs": [],
   "source": [
    "# the feature matrix\n",
    "X = yaz.data\n",
    "\n",
    "# the target variables\n",
    "y = yaz.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqYQdDSbqCaj"
   },
   "source": [
    "As we want to compare different models in the course of this tutorial, we have to split the data into training and test set. While we use the training set to build a model, we need the test set to evaluate it on unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "beem-TByq5Ft"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ietJFQ6iL4Ps"
   },
   "source": [
    "Note that some models that we are going to use cannot handle categorical features. For this reason, we also load a one-hot encoded version of feature matrix $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TLvRFtfWnknB"
   },
   "outputs": [],
   "source": [
    "X_encoded = load_yaz(include_prod=[\"steak\"], one_hot_encoding=True).data\r\n",
    "X_train_encoded, X_test_encoded = train_test_split(X_encoded, train_size=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtSg8QV8SRi8"
   },
   "source": [
    "In addition,  we normalize the feature matrix, since some models may be sensitive to variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5A2QslfKS0A7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "scaler = StandardScaler()\r\n",
    "scaler.fit(X_train_encoded)\r\n",
    "X_train_scaled = scaler.transform(X_train_encoded)\r\n",
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JEmFUO-F0pq"
   },
   "source": [
    "## Parametric Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctHsqVk7Dm_N"
   },
   "source": [
    "Let us now use the Yaz dataset to determine the optimal inventory quantity of steak for a given day. One way we can use the data is to estimate the true demand distribution based on the past demand samples. To do this, we first explore the data with a simple histogram. For plotting we use `seaborn` - a Python data visualization library.       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Uylmg0qy0s-c",
    "outputId": "53bdb4e7-e4d4-4ed0-86dc-f209576116bf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram \n",
    "sns.distplot(y_train, hist=True, norm_hist=False, kde=False,\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Demand')\n",
    "plt.show(sns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBofa05UWIae"
   },
   "source": [
    "As we can see, it looks like the demand follows a normal distribution. We therefore estimate mean $\\mu$ and standard deviation $\\sigma$ to fit a normal distribution to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "KAb0o8wR2wuN",
    "outputId": "9beba730-e8f7-4c74-d911-be4e785362da"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# determine mean and standard deviation\n",
    "mean = round(y_train.mean(),2)\n",
    "std = round(y_train.std(),2)\n",
    "\n",
    "# plot histogram and fit a normal distribution to it\n",
    "sns.distplot(y_train, hist=True, norm_hist=False, fit=norm, \n",
    "             kde=False, hist_kws={'edgecolor':'black'})\n",
    "\n",
    "plt.ylabel('Empirical probability')\n",
    "plt.xlabel('Demand')\n",
    "plt.title(\"Normal distribution with mean = %.2f and std = %.2f\" % (mean, std))\n",
    "\n",
    "plt.show(sns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM03NIn6F8ho"
   },
   "source": [
    "In the next step, we can use this distribution to determine how many steaks to defrost overnight. But before that, we have to define the under- and overage costs for steak. The store manager tells us that each unit of unsold steak costs 5€ and each unit of demand that cannot be met costs 15€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Urydk7CT0HO_"
   },
   "outputs": [],
   "source": [
    "cu = 15\n",
    "co = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZukLd2MylP_"
   },
   "source": [
    "Finally, we can implement equation (2), which then tells us how many steaks to defrost for a single day (the optimal inventory quantity $q$). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP1Ah3wW07Wz",
    "outputId": "15d36efc-0d07-48ca-a1ba-67e5d24f1b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine optimal inventory quantity\n",
    "q = norm(mean,std).ppf(cu/(cu+co))[0]\n",
    "round(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZPMJrvC6HlO"
   },
   "source": [
    "We call this the traditional parametric approach, we first assume the demand falls in a family of parametric distributions, estimate its parameters based on past demand samples, and then solve the initial optimization problem (2). \n",
    "\n",
    "To get a representation of the decision quality, we calculate the average costs over all days in the test set. For this, we use the `average_costs` function implemented in `ddop`. The function takes four arguments: the true values, the predicted values, the underage costs, and the overage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iarxlf2p4xJg",
    "outputId": "1dd85001-9aa0-4620-b690-04749fcbad1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.46472249886655"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array with length equal to number of observations in test set filled with the value of the inventory quantity q\n",
    "y_pred = np.full(y_test.shape[0],q)\n",
    "\n",
    "# calculate average costs\n",
    "average_costs(y_true=y_test, y_pred=y_pred, cu=cu, co=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z57p1eTR8jFL"
   },
   "source": [
    "As you can see, we have average costs of 66.46€ associated with our decision. But can we do better? To answer this, let us go back to our data. So far we only used past steak demand samples. However, we have access to exogenous features that may have predictive power for demand, e.g. the weekday.  So next, we investigate whether the day of the week has an impact on demand. To do this, we determine mean and standard deviation for each weekday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "Jud6L45xui_O",
    "outputId": "4ef2854d-babd-41b9-a351-1ab1221fa680"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FRI</th>\n",
       "      <td>26.43</td>\n",
       "      <td>8.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MON</th>\n",
       "      <td>18.79</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT</th>\n",
       "      <td>37.80</td>\n",
       "      <td>12.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUN</th>\n",
       "      <td>16.55</td>\n",
       "      <td>6.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THU</th>\n",
       "      <td>21.89</td>\n",
       "      <td>6.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TUE</th>\n",
       "      <td>20.27</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WED</th>\n",
       "      <td>21.80</td>\n",
       "      <td>6.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Mean    Std\n",
       "weekday              \n",
       "FRI      26.43   8.06\n",
       "MON      18.79   7.44\n",
       "SAT      37.80  12.21\n",
       "SUN      16.55   6.08\n",
       "THU      21.89   6.79\n",
       "TUE      20.27   6.33\n",
       "WED      21.80   6.54"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group demand by weekday\r\n",
    "data = pd.concat([X_train, y_train], axis=1)\r\n",
    "data_by_weekday = data.groupby(\"weekday\")[\"steak\"]\r\n",
    "\r\n",
    "# determine mean and standard deviation for each weekday\r\n",
    "mean_by_weekday = data_by_weekday.mean()\r\n",
    "std_by_weekday = data_by_weekday.std()\r\n",
    "d = {'Mean':mean_by_weekday, 'Std':std_by_weekday}\r\n",
    "mean_and_std_by_weekday = pd.DataFrame(d).round(2)\r\n",
    "mean_and_std_by_weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eWku2aS96sm"
   },
   "source": [
    "We can clearly see that the demand varies greatly depending on the day of the week. We can clearly see that demand varies greatly depending on the day of the week. While the highest amount of steak is sold on Saturdays, the least amount is sold on Sundays and Mondays. One way to take this information into account is to fit a normal distribution for the samples of each weekday, respectively. As before, we can then use these distributions to determine the optimal inventory quantity by solving equation (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NciMvO_2D6zB",
    "outputId": "9b4dca83-a4f8-4a46-e3fc-ebcfa5fe0c35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FRI': 32.0,\n",
       " 'MON': 24.0,\n",
       " 'SAT': 46.0,\n",
       " 'SUN': 21.0,\n",
       " 'THU': 26.0,\n",
       " 'TUE': 25.0,\n",
       " 'WED': 26.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine decision q depending on the weekday\r\n",
    "days = ['MON','TUE','WED','THU','FRI','SAT','SUN']\r\n",
    "q_by_weekday = {}\r\n",
    "for day in days:\r\n",
    "  mean = mean_and_std_by_weekday.loc[day]['Mean']\r\n",
    "  std = mean_and_std_by_weekday.loc[day]['Std']\r\n",
    "  q_by_weekday[day] = round(norm(mean, std).ppf(cu/(cu+co)))\r\n",
    "q_by_weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSHo9xbaHreO"
   },
   "source": [
    "Given the optimal decision for each weekday, we can write a simple predict function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ht2OgCmgygZc"
   },
   "outputs": [],
   "source": [
    "def predict(X):\r\n",
    "  \"\"\"\r\n",
    "  Get the decision for each sample in X depending on the weekday.\r\n",
    "\r\n",
    "  Parameters\r\n",
    "  ----------\r\n",
    "  X : Pandas DataFrame of shape (n_samples, n_features)\r\n",
    "      The input samples. \r\n",
    "\r\n",
    "  Returns\r\n",
    "  ----------\r\n",
    "  y : array of shape (n_samples,)\r\n",
    "      The predicted values.\r\n",
    "  \"\"\"\r\n",
    "\r\n",
    "  pred = [] \r\n",
    "  for index, sample in X.iterrows():\r\n",
    "    day = sample['weekday']\r\n",
    "    pred.append(q_by_weekday[day])\r\n",
    "\r\n",
    "  return np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD5FGO3t3XFY"
   },
   "source": [
    "We apply the function to the test set and calculate the average costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yokIMohOqExa",
    "outputId": "c43977b2-99ed-4b75-eb90-b6350a7836aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.078947368421055"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict optimal inventory quantity\n",
    "y_pred = predict(X_test)\n",
    "\n",
    "# calculate average costs\n",
    "average_costs(y_test, y_pred, cu, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skezZM0DAGOk"
   },
   "source": [
    "Now look, we were able to reduce the average costs from 66.46€ to 54.08€. That is a great improvement! But maybe we can do even better by considering the other features of the dataset as well. To do this, we could group observations with same feature values and then estimate a normal distribution for each group - like we did before but now with more features than just the weekday. However, this has two main drawbacks:\n",
    "1. Imagine we have two features, the weekday and the month. If we were to estimate a distribution for each feature combination, we would have to fit a total of 7*12=84 distributions. That sounds like a lot of work, doesn't it?\n",
    "2. To stay with the example: We consider the case where it is Monday and January. As you can see below, we only have 8 samples with the same feature values. Such a small number of samples makes it hard to fit a distribution. Moreover, with an increasing number of features we may not have a single sample with the same feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQJbsPwq_cQ3",
    "outputId": "fa264705-a613-425a-bc4e-f59126aa38cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[(X_train['weekday']=='MON')&(X_train['month']==\"JAN\")].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-FkKCMm6i-P"
   },
   "source": [
    "Instead of estimating a distribution for each group of samples with the same feature values, we can train a machine learning model to predict the demand. For example, we can use the `DecisionTreeRegressor` from `sklearn`. Note that we set the maximum depth of the tree to 10 to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XuWbYKIjCXeH"
   },
   "outputs": [],
   "source": [
    "# predict demand using a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor(max_depth=10,random_state=1)\n",
    "DT.fit(X_train_scaled,y_train)\n",
    "pred = DT.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsPV6abmCYG5"
   },
   "source": [
    "Of course we cannot assume the model to be perfect: first, because of the model error itself, and second, because of the uncertainty in demand. For this reason, we need to adjust the predictions for uncertainty to obtain optimal decisions. We can get a representation of the remaining uncertainty by estimating the distribution of the prediction error on the training set. Assuming the error to be normally distributed, we determine the parameters $\\mu_{e}$ and $\\sigma_{e}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "muqTYTeCF5Q9"
   },
   "outputs": [],
   "source": [
    "# determine mean and standard deviation of the prediction error\n",
    "error = y_train[\"steak\"]-DT.predict(X_train_scaled)\n",
    "error_mean = error.mean()\n",
    "error_std = error.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na82qoObF5rC"
   },
   "source": [
    "Next, we pass the error distribution to equation (2) to determine an additional safety buffer that adjusts the predictions based on the forecast error by balancing the expected overage and underage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "b1TTv4u2uqGH"
   },
   "outputs": [],
   "source": [
    "# determine safety buffer\r\n",
    "safety_buffer = norm(error_mean, error_std).ppf(cu/(cu+co))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z3gLI8ruuNN"
   },
   "source": [
    "The final order decision is then the sum of both the prediction generated by our model and the safety buffer. More formally, the solution to the newsvendor problem can then be stated as:\r\n",
    "\r\n",
    "\\begin{equation}\r\n",
    "q(x)^{*} = \\mu(x)+\\Phi^{-1}(\\alpha),\r\n",
    "\\tag{3}\r\n",
    "\\end{equation}\r\n",
    "\r\n",
    "where $\\mu(\\cdot)$ is the function of the machine-learning model that predicts the demand given feature vector $x$, and is $\\Phi^{-1}$ the inverse cdf of the error distribution with parameter $\\mu_{e}$ and $\\sigma_{e}$. Consequently, in the next step, we add the safety buffer to the predictions to obtain optimal inventory quantities. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-v2L8FlEdnHn"
   },
   "outputs": [],
   "source": [
    "# add safety buffer to the predictions of the model \n",
    "pred = pred + safety_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTxF9yJYxuDO"
   },
   "source": [
    "Finally, we calculate the corresponding average costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TtTcJY-xthM",
    "outputId": "63a98c64-a467-4ace-8851-447e497da43b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.668449152885316"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate average costs \r\n",
    "average_costs(y_test, pred, cu, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlUAYHl5H5XE"
   },
   "source": [
    "Again, we were able to reduce our average costs from 54.08€ to 51.67€. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNvGez2XKm18"
   },
   "source": [
    "### Summary\n",
    "Let us now summarize what we have learned so far:\n",
    "\n",
    "* In case we do not know the demand distribution, we can estimate the distribution based on past demand samples and then solve equation (2) to determine the optimal inventory quantity.\n",
    "* We can improve the decision by taking features into account. To do this, we group the data based on their feature values and estimate a distribution for each group. However, this does not work well for a large number of features.  \n",
    "* Instead, we can use a machine-learning model to predict demand. Since we cannot assume the model to be perfect, we estimate the distribution of the prediction error to calculate an additional safety buffer. The optimal decision is then the sum of prediction and safety buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A4I14kbGLoI"
   },
   "source": [
    "## Data-Driven Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62xAo8ImGQDo"
   },
   "source": [
    "So far we have only considered the traditional parametric way to solve the newsvendor problem. In this part of the tutorial, we introduce different **\"data-driven\"** approaches to solve the newsvendor problem. In contrast to the traditional way of first estimating the demand distribution and then solving the initial optimization problem, these approaches directly prescribe decisions from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YREycTQGZpm"
   },
   "source": [
    "### Sample Average Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ATO07tU1Po"
   },
   "source": [
    "The simplest **data-driven** approach to solve the newsvendor problem is called **Sample Average Approximation (SAA)**. The idea behind **SAA** is to determine the optimal inventory quantity by finding the decision $q$ that minimizes the average cost on past demand samples. Formally, the optimization problem can be stated as follows: \r\n",
    "\r\n",
    "\\begin{equation}q^{*}=\\min _{q \\geq 0} \\frac{1}{n} \\sum_{i=1}^{n}\\left[c u\\left(d_{i}-q\\right)^{+}+c o\\left(q-d_{i}\\right)^{+}\\right]\r\n",
    "\\tag{4}\r\n",
    "\\end{equation}\r\n",
    "\r\n",
    "where $n$ is the total number of samples and $d_i$ is the i-th demand observation. Moreover, $(\\cdot)^+$ is a function that returns $0$ if its argument is negative, and else its argument. Using this function we ensure that we multiply missing units by the underage costs and excess units by the overage costs. To fully understand how **SAA** works, we go through a simple example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsEYLBSppAf6"
   },
   "source": [
    "**Example:** Determine how many steaks to defrost by applying **SAA**. The past demand for steak is given by: $D=[27,29,30]$. You can sell steak in your restaurant for 15€ (underage costs), while unsold units incur overage costs of 5€.\n",
    "\n",
    "In the following, we try to minimize the optimization problem (4). For simplicity, we do not write down both terms $cu(d_i-q)^+$ and $co(q-d_i)^+$. One of them is always zero because we can have either underage units or overage units.\n",
    "\n",
    "\\begin{equation}\n",
    ".....\\\\\n",
    "q=27: \\frac{1}{3}\\Bigl[15*(27-27)+15*(29-27)+15*(30-27)\\Bigl]=25\\\\\n",
    "q=28: \\frac{1}{3}\\Bigl[5*(28-27)+15*(29-28)+15*(30-28)\\Bigl]=16,67\\\\\n",
    "q=29: \\frac{1}{3}\\Bigl[5*(29-27)+15*(29-29)+15*(30-29)\\Bigl]=8,33\\\\\n",
    "q=30: \\frac{1}{3}\\Bigl[5*(30-27)+5*(30-29)+15*(30-30)\\Bigl]=6,67\\\\\n",
    "q=31: \\frac{1}{3}\\Bigl[5*(31-27)+5*(31-29)+5*(31-30)\\Bigl]=11,67\\\\\n",
    ".....\n",
    "\\end{equation}\n",
    "\n",
    "Based on our calculation, the optimal decision is given by $q=30$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0_quzqbpQoM"
   },
   "source": [
    "Now that we know how **SAA** works, we can apply the approach on the Yaz dataset by using the class `SampleAverageApproximationNewsvendor` from `ddop`. To initialize the model we pass the under- and overage costs to the constructor. Subsequently, we fit the model to the past demand samples and determine the optimal decision using the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D7KackKPB0G",
    "outputId": "e3f47a3d-1f6a-4632-bda6-e6f7a628caba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28]])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAA = SampleAverageApproximationNewsvendor(cu,co)\n",
    "SAA.fit(y_train)\n",
    "SAA.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBsH30e35L1D"
   },
   "source": [
    "Based on **SAA** the optimal decision is 28. To get a representation of the model performance, we use the model's `score` function. Just like the `average_costs` function, the `score` method calculates the average cost. The advantage is that we can directly calculate the costs without having to predict all training samples first.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFX5j_3g1Rx9",
    "outputId": "2559f192-9193-435f-b0e4-8e4f300f551a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.23684210526316"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-SAA.score(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msghOUdjE1bo"
   },
   "source": [
    "With average cost of 60.24€, **SAA** performs better compared to the traditional parametric approach without features (66.46€). In contrast, **SAA** performs worse compared to the parametric approach with features (51.67€). However, this is not really surprising since we already know that exogenous features can improve decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIEmJgSfGkzJ"
   },
   "source": [
    "### Weighted Sample Average Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiHpsuo31SC5"
   },
   "source": [
    "Even though **SAA** is a common and effective approach, it only considers past demand samples. However, in the first part of the tutorial we have seen that we can improve our decision by taking into account features such as the weekday. We now want to do the same here by determining the optimal **SAA** decision for each weekday separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7Juk1K8Wu5Z",
    "outputId": "62acc290-7d5e-4a8c-b41f-b2bfe99c1515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MON': 21, 'TUE': 24, 'WED': 26, 'THU': 26, 'FRI': 31, 'SAT': 45, 'SUN': 20}\n"
     ]
    }
   ],
   "source": [
    "# determine q depending on the weekday using SAA\n",
    "q_by_weekday = {}\n",
    "\n",
    "SAA = SampleAverageApproximationNewsvendor(cu,co)\n",
    "\n",
    "for day in days:\n",
    "  demand = data_by_weekday.get_group(day)\n",
    "  SAA.fit(demand)\n",
    "  q_by_weekday[day] = SAA.predict().item(0)\n",
    " \n",
    "print(q_by_weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwpv3QD7nUFE"
   },
   "source": [
    "Given the optimal decision for each day, we can write a new prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "L_MhbDPNb_mX"
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "  \"\"\"\n",
    "  Get the prediction for each sample in X depending on the weekday.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  X : Pandas DataFrame of shape (n_samples, n_features)\n",
    "      The input samples.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  y : array of shape (n_samples,)\n",
    "      The predicted values.\n",
    "  \"\"\"\n",
    "\n",
    "  pred = [] \n",
    "  for index, sample in X.iterrows():\n",
    "    day = sample['weekday']\n",
    "    pred.append(q_by_weekday[day])\n",
    "\n",
    "  return np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y75G3Un1up-x"
   },
   "source": [
    "We can then apply the function on the test set and calculate the average costs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uiv1QLgxc8jK",
    "outputId": "307a82e1-18bb-4ef4-933f-8a1cbef1d77d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.23684210526316"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(X_test)\n",
    "average_costs(y_test,pred,cu,co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjkWm0xqvecJ"
   },
   "source": [
    "Again, we were able to reduce the average costs from 60.24€ to 52.24€ by taking into account the weekday. We can now go a step further by considering more features. To do this, we could use a separate model for the samples with the same feature values. However, as we have seen in the traditional parametric approach, this can become problematic when we have only a small number of samples with the same feature values. Another way to take feature information into account is to determine a weight for each sample based on the similarity to a new instance and optimize **SAA** against a re-weighting of the data. To determine the sample weights, we can use different machine learning techniques, such as a regression tree. For a better understanding, consider the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qF2qpTTN6Yx"
   },
   "source": [
    "**Example:** Say we have the following past demand samples with feature vector $x_i=(Weekend, Temperature)$ and we want to calculate their weights based on a new sample $x=(0,18)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2JrfBh9UOZn"
   },
   "source": [
    "| Sample| Weekend | Temperature [in °C] | Demand |\n",
    "|:-:|:-:|:--:|:--:|\n",
    "| 1| 0 | 19 | 27 |\n",
    "| 2| 1 | 25 | 29 |\n",
    "| 3| 1 | 23 | 30 |\n",
    "| 4| 0 | 25 | 18 |\n",
    "| 5| 0 | 24 | 20 |\n",
    "| 6| 0 | 22 | 23 |\n",
    "| 7| 0 | 11 | 21 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI3sa4qWUpBe"
   },
   "source": [
    "First, we train a regression tree to predict the demand given the features weekday and temperature. The resulting tree looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5Lo_EuSKnM"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1fXW6PV2bUcnZSNByTde39k8hEgRB_5aV\" width=\"450\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xysvlCNFN9qy"
   },
   "source": [
    "The regression tree splits our data into four leafs based on the two features. \n",
    "To determine the sample weights, we look to which leaf the new instance $x=(0,18)$ belongs. In the first level of the tree we follow the left path since it is no weekend. Then, given the temperature forecast of $18°C$, we end up in leaf 1 together with sample 1, 6, 7. This means that these observations are most similar to $x$. Using this information, we assign a weight of $\\frac{1}{3}$ to each of the three samples falling into the same leaf. The sample weights are then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "w_1=\\frac{1}{3},\\:w_2=0,\\:w_3=0,\\:w_4=0,\\:w_5=0,\\:w_6=\\frac{1}{3},\\:w_7=\\frac{1}{3}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5LqcvxVON2"
   },
   "source": [
    "Now that we have calculated the sample weights, we can solve the optimization problem like before. The only difference is that we multiply each observation with its corresponding weight, which indicates the similarity to the new sample.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEueZzmfJQqT"
   },
   "source": [
    "\\begin{equation}\n",
    "q=18: \\frac{1}{3}\\bigl[15*(27-18)\\bigl]+...+\\frac{1}{3}\\bigl[15*(23-18)\\bigl]+\\frac{1}{3}\\bigl[15*(21-18)\\bigl] = 85\\\\\n",
    "......\\\\\n",
    "q=26: \\frac{1}{3}\\bigl[15*(27-26)\\bigl]+...+\\frac{1}{3}\\bigl[5*(26-23)\\bigl]+\\frac{1}{3}\\bigl[5*(26-21)\\bigl] = 18.33\\\\ \n",
    "q=27: \\frac{1}{3}\\bigl[15*(27-27)\\bigl]+...+\\frac{1}{3}\\bigl[5*(27-23)\\bigl]+\\frac{1}{3}\\bigl[5*(27-21)\\bigl] = 16.67\\\\ \n",
    "q=28: \\frac{1}{3}\\bigl[5*(28-27)\\bigl]+...+\\frac{1}{3}\\bigl[5*(28-23)\\bigl]+\\frac{1}{3}\\bigl[5*(28-21)\\bigl] = 21.67\\\\\n",
    "......\\\\\n",
    "q=30: \\frac{1}{3}\\bigl[5*(30-27)\\bigl]+...+\\frac{1}{3}\\bigl[5*(30-23)\\bigl]+\\frac{1}{3}\\bigl[5*(30-21)\\bigl] = 30.67 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6J0RYXdCF8w"
   },
   "source": [
    "Based on our calculation, $q=27$ is the quantity  minimizing the average costs on the weighted samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KUZ4-wGF542"
   },
   "source": [
    "We call this approach **\"weigthed Sample Average Approximation (wSAA)\"** as it can be seen as weighted form of **SAA**. More formally the problem can be stated as follows:  \n",
    "\n",
    "\\begin{equation}\n",
    "q(x)^*=\\min_{q\\geq 0} \\sum_{i=1}^{n}w_i(x)\\bigl[cu(d_i-q)^+ + co(q-d_i)\\bigl],\n",
    "\\tag{5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA6nYpKnFfiM"
   },
   "source": [
    "where $x$ is the feature vector of a new instance and $w_i(\\cdot)$ is a function that assigns a weight $w_i\\in [0,1]$ to each past demand sample $S_i(x_i,d_i)$ based on the similarity of $x$ and $x_i$. In the example above we used a weight function based on a decision tree (DT), given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAbiva-OMVJE"
   },
   "source": [
    "\\begin{equation}\n",
    "w_{i}^{Tree}(x)=\\frac{\\mathbb{1}x_i \\in R(x)}{N(x)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbb{1}$ is the indicator function, $R(x)$ is the leaf containing $x$, and $N(x)$ is the number of samples falling into leaf $R(x)$. In other words, the function will give a weight of $\\frac{1}{N(x)}$ to sample $i$ if $x_i$ belongs to the same leaf as the new instance $x$, and otherwise zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob_e8o2hPkfd"
   },
   "source": [
    "Instead of using a DT, we can also use other machine learning methods to determine the sample weights, for example k-nearest-neighbors (kNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suiz1md_TxZ4"
   },
   "source": [
    "\\begin{equation}\n",
    "w_{i}^{k \\mathrm{NN}}(x)=\\frac{1}{k} \\mathbb{1}\\left[x_i \\text { is a } k \\mathrm{NN} \\text { of } x\\right],\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grIWKTo3NLWX"
   },
   "source": [
    "where $k$ is the number of neighbors. In simple terms, the function will give a weight of $\\frac{1}{k}$ to sample $x_i$ if it is a k-nearest-neighbor of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAalmUi-klsD"
   },
   "source": [
    "We can apply these approaches on our dataset by using the class `RandomForestWeightedNewsvendor` and `KNeighborsWeightedNewsvendor` from `ddop`. To initialize the models, we pass the overage- and underage costs to the constructor. Moreover, we define some addition model-specific parameters. For instance, we set the maximum depth for `DecisionTreeWeightedNewsvendor` to $8$ and the number of neighbors for `KNeighborsWeightedNewsvendor` to $30$. After we have initialized our models we fit them on the training data and calculate the average costs on the test set by using the `score` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40nTuNuMPVur",
    "outputId": "3d7cb570-a799-483b-d98e-17000fdcb633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. cost DTW:  47.6578947368421\n",
      "Avg. cost kNNW:  47.473684210526315\n"
     ]
    }
   ],
   "source": [
    "DTW = DecisionTreeWeightedNewsvendor(cu=cu, co=co, max_depth=8,random_state=1)\n",
    "DTW.fit(X_train_scaled, y_train[\"steak\"])\n",
    "DTW_score = DTW.score(X_test_scaled,y_test)\n",
    "print(\"Avg. cost DTW: \",-DTW_score)\n",
    "\n",
    "kNNW = KNeighborsWeightedNewsvendor(cu=cu, co=co, n_neighbors=30)\n",
    "kNNW.fit(X_train_scaled, y_train)\n",
    "kNNW_score = kNNW.score(X_test_scaled,y_test)\n",
    "print(\"Avg. cost kNNW: \", -kNNW_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_iXS1Cb9LZj"
   },
   "source": [
    "These are the best results so far. Both models, **DTW** (47.66€) and **kNNW** (47.47€), outperform all other models we have seen so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmYRd408-ZwX"
   },
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bMErETw_NnO"
   },
   "source": [
    "### Empirical Risk Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t4w6Uf091Fj"
   },
   "source": [
    "With **wSAA** we have already learned a data-driven approach that is able take exogenous features into account. However, to obtain a decision $q$, we have to first determine weights and then solve an optimization problem for every new sample $x$. But wouldn't it be great to learn a function that instead maps directly from features $x\\in X$ to a decision $q\\in Q$? A natural way to obtain such a function is though [empirical risk minimization](https://en.wikipedia.org/wiki/Empirical_risk_minimization):\r\n",
    "\\begin{equation}\r\n",
    "\\min_{q(\\cdot)\\in\\mathcal{F}} \\frac{1}{n}\\sum_{i=1}^{n}\\bigl[cu(d_i-q(x_i))^+ + co(q(x_i)-d_i)^+\\bigl],\r\n",
    "\\tag{6}\r\n",
    "\\end{equation}\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4UbO-B2AYYh"
   },
   "source": [
    "where $q(\\cdot)$ is the function that maps from feature space $X$ to decision space $Q$, and $\\mathcal{F}$ is its function class. Again, $x_i$ is the feature vector of the i-th sample, and $d_i$ is the corresponding demand value. To say it simple, we try to find the function $q(\\cdot):X\\rightarrow Q$ that minimizes the average costs on $n$ past demand samples (the empirical risk). The most straightforward way to do this is based on linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqikaLTcSzC8"
   },
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0UAFV1i-_0B"
   },
   "source": [
    "Suppose we have only a single feature, say temperature, and given the value for the temperature we want to know how many steaks to defrost.  In other words, we want to find a function $q(temp)$ that maps from temperature to a decision $q$. Of course, the first thing we need to do is  to collect some data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7l9ajNvDZkV"
   },
   "source": [
    "| Sample| Temperature [in °C] | Demand |\r\n",
    "|:-:|:-:|:--:|\r\n",
    "| 1| 5 | 16 |\r\n",
    "| 2| 7 | 12 |\r\n",
    "| 3| 10 | 14 |\r\n",
    "| 4| 12 | 10 |\r\n",
    "| 5| 15 | 11 |\r\n",
    "| 6| 18 | 7 |\r\n",
    "| 7| 20 | 9 |\r\n",
    "| 8| 21 | 6 |\r\n",
    "| 9| 23 | 8 |\r\n",
    "| 10| 25 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD76KXncFqQY"
   },
   "source": [
    "We plot the data and it looks like there is a linear relationship between temperature and demand (see figure below). Consequently, to solve our problem the first thing that comes into mind is linear regression. For this reason, we construct a linear function $q(temp)=b+w*temp$, where $b$ is the intercept term, and $w$ is a weight. We can think of the intercept as a base demand to which we add or subtract a certain amount depending on the temperature and weight $w$. To find the optimal mapping from temperature to demand, we now have to learn the best fitting values for $b$ and $w$. We start by setting $b=16$ and $w=-0,4$, which gives us function $q(temp)=16-0,4*temp$ that already fits the data quite well. Still, for each sample $i$ we obtain an error representing the difference between the actual demand $d_i$ and the estimated decision of $q(temp_i)$. In the figure below these errors are illustrated as dotted lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkl9v5ibJzfd"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1IbPX_WRmSURbEypcyYdwwcvSo7aUjfDZ\" width=\"600\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAx7-WH6_Zwr"
   },
   "source": [
    "According to linear regression, the goal would be to adjust the values for $b$ and $w$ in a way that minimizes the mean difference (error) between the actual demand and the estimated decision. Formally speaking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlEs1hyKX-I6"
   },
   "source": [
    "\\begin{equation}\r\n",
    "\\min_{b,w} \\frac{1}{n}\\sum_{i=1}^{n}\\Bigl(d_i-(b+w*temp)\\Bigl)^2\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3V-ef7EYUuB"
   },
   "source": [
    "In our case, however, this can be problematic because overage would have the same effect as underage. Now recall that each unit of unsold steak costs 5€, while each unit of demand that cannot be satisfied costs 15€. In other words, one unit too little costs three times more than one unit too much. Consequently, we minimize the average cost rather than the mean difference. The resulting optimization problem is then given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiYxwK22BV7F"
   },
   "source": [
    "\\begin{equation}\r\n",
    "\\min_{b,w} \\frac{1}{n}\\sum_{i=1}^{n}\\bigl[cu(d_i-(b+w*temp))^+ + co((b+w*temp)-d_i)^+\\bigl]\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BF0744hbLVP"
   },
   "source": [
    "Doesn't this look like equation $(6)$, which we introduced at the very beginning of this chapter? The only difference is that we have defined $q(\\cdot)$ as a linear decision function of the form $q(temp)=b+w*temp$. To solve this optimization problem, we can use the class `LinearRegressionNewsvendor` from `ddop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byxC7JhQPulz",
    "outputId": "882b8d1d-8ce3-419c-83e2-29886a522130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionNewsvendor(co=5, cu=15)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [[5],[7],[10],[12],[15],[18],[20],[21],[23],[25]]\r\n",
    "demand = [16,12,14,10,11,7,9,6,8,4]\r\n",
    "mdl = LinearRegressionNewsvendor(cu=15,co=5)\r\n",
    "mdl.fit(X=temp,y=demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0x2h5NaP4bS"
   },
   "source": [
    "After fitting the model, we can access the intercept term $b$ via the argument `intercept_` and the weight $w$ via the argument `feature_weights_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uKHnY47Rqfu",
    "outputId": "b29419ca-34d5-42c1-fea4-33941ee4bb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: [18.333333]\n",
      "w: [[-0.46666667]]\n"
     ]
    }
   ],
   "source": [
    "print(\"b:\",mdl.intercept_)\r\n",
    "print(\"w:\",mdl.feature_weights_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp6Cix65Ue1I"
   },
   "source": [
    "Thus, the function $q(\\cdot)$ that fits our data best is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPuvIZb2VO2w"
   },
   "source": [
    "\\begin{equation}\r\n",
    "q(temp)=18.3-0.47*temp\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i19a2ERIWB6x"
   },
   "source": [
    "In the next step we want to know how many steaks to defrost for tomorrow. We check the weather forecast, which tells us that it will be 25 degrees. Given the temperature, the function $q(\\cdot)$ then tells us that the optimal inventory stock is: $18.3-0.47*10=13.6$. Instead of determining the decision ourselves, we can also use the model's ´predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BOFGQYYXvRR",
    "outputId": "a075a1bf-4e62-4f94-cf12-13ab78e5aa9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.6666663]])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.predict([[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAQb_lFNNVeF"
   },
   "source": [
    "Of course, this was just a simple example to illustrate the concept of this approach. As we know, the Yaz dataset provides a lot more features than just the temperature. Consequently, we are looking for a function of the form:\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzMrRbkQiJgw"
   },
   "source": [
    "\\begin{equation}\r\n",
    "q(x)=b+w_1*x_1+...+w_m*x_m=b+\\sum_{j=1}^{m}w_j*x_j,\r\n",
    "\\tag{7}\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vChUo4t-rFxv"
   },
   "source": [
    "where $x_j$ represents the value of feature $j$-th from sample $x$, and $w_j$ is the corresponding feature weight. As a result, the optimization problem that we want to solve becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbXqWP_6tO3X"
   },
   "source": [
    "\\begin{equation}\r\n",
    "\\min_{b,w_1,..,w_m} \\frac{1}{n}\\sum_{i=1}^{n}\\bigl[cu(d_i-b-\\sum_{j=1}^{m}w_j*x_{i,j})^+ + co(b+\\sum_{j=1}^{m}w_j*x_{i,j}-d_i)^+\\bigl],\r\n",
    "\\tag{8}\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuVx0VzvTcOh"
   },
   "source": [
    "We now have to learn the value for intercept term $b$ and feature weights $w_1,...,w_m$. For now, we do this just for the temperature and the weekdays. Note that we use the one hot encoded version of the data, thus each weekday is represented by a binary column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMMhv-fsNi49",
    "outputId": "8dccf0a6-62c6-4522-b492-44352808eb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.860215]\n",
      "[[-0.21505376  0.          2.2580645   3.7311828   2.9032258   8.3978495\n",
      "  22.225806   -1.5913978 ]]\n"
     ]
    }
   ],
   "source": [
    "cols = ['temperature', 'weekday_MON', 'weekday_TUE', 'weekday_WED', 'weekday_THU', 'weekday_FRI', 'weekday_SAT', 'weekday_SUN']\r\n",
    "LRN = LinearRegressionNewsvendor(cu=15,co=5)\r\n",
    "LRN.fit(X_train_encoded[cols],y_train)\r\n",
    "print(LRN.intercept_)\r\n",
    "print(LRN.feature_weights_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfr9r-hMeKYF"
   },
   "source": [
    "We note that the intercept term is 24.86. This is now our base demand to which we add or subtract a certain amount, depending on corresponding feature values. For example, we add 22.23 for a Saturday. In contrast, we subtract 1.59 for a Sunday. The weight for the temperature is -0.215 so we would subtract 2.15 in case it is 10°C. At this point it should be clear how this approach works and how the parameters are to be interpreted. So in the next step, we can now apply the model to our dataset. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HEmZ9iAmlk6",
    "outputId": "fc271fcd-85b2-4e6a-85df-603e8b44c1e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.932357245389476"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRN = LinearRegressionNewsvendor(cu=15,co=5)\r\n",
    "LRN.fit(X_train_encoded,y_train)\r\n",
    "-LRN.score(X_test_encoded,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgYKuDxQZbid"
   },
   "source": [
    "With average costs of €48.93 the model performs slightly worse compared to the **wSAA** approaches. However, it outperforms all other models we have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHf0VRI_aZxe"
   },
   "source": [
    "#### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwVF7OFEcroO"
   },
   "source": [
    "In the previous section, we assumed that the demand is a linear combination of features. To find the perfect mapping from features $x$ to decision $q$ we therefore specified that $q(\\cdot)$ belongs to the class of linear decision functions. However, sometimes there is no linear relationship, so we need a nonlinear function to describe our data. One way to obtain such a function is by using a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLpyTsUOOvF2"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1rmgdo9urd4Qx5MQrPu4sO9sto8_ogCm1\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_-Us5gRktd9"
   },
   "source": [
    "A DNN uses a cascade of many layers to obtain an output given some input data. In general, it can be distinguished between input-, hidden-, and output-layer, each consisting of a number of neurons. In the first layer the number of neurons corresponds to the number of inputs. In other words, each neuron takes the value of a single feature as input, e.g. the temperature or the weekday. The input-layer is followed by a number of hidden-layers, each presented by an arbitrary number of neurons. In the output-layer the number of neurons corresponds to the number of outputs. In our example, we only have a single neuron that outputs the decision $q$ conditional on the features temperature and weekday. The individual neurons of a layer are each connected to the neurons of the layer before and behind. In a graph, the neurons can be represented as nodes and their connections as weighted edges. A neuron takes the outputs of the neurons from the previous layer as inputs. Subsequently, it computes the weighted sum of its inputs and adds a bias to it. Formally speaking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWHieBeT1kva"
   },
   "source": [
    "\\begin{equation}\r\n",
    "bias+\\sum_{l=1}^{L}x_l*w_l, \r\n",
    "\\tag{9}\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRysshwD16Nq"
   },
   "source": [
    "where $x_l$ is the $l$-th input of a neuron and $w_l$ the corresponding weight. Does this look familiar? This is the exactly the decision function $(7)$ that we used in the regression based approach before. The only difference is that we use the term \"bias\" for the constant value instead of \"intercept\". But what does this mean? If we were just to combine a set of linear functions, we would get a single linear function as a result. In other words, there would be no difference to the previous approach. This is where the activation function comes into play. The activation function is a non-linear function that transforms the computed value of equation (9) and then outputs the final result of a neuron. For example, the Rectified Linear Unit (ReLU) activation function outputs $0$ if the input value is negative and its input otherwise. Thus, the DNN models a piecewise linear function, which may looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2HciRoCF_3D"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=18YRxB6jYlqV97FaEPXf-IcRuJt1nWkWI\"  width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylaUwdjoYUx"
   },
   "source": [
    "The goal of the network is then to find the function that fits the data best. To find such a function in the linear regression based model, we had to determine the optimal values for the feature weights and the intercept term. This is basically the same here. We just have a lot more weights and intercept terms (biases). Since we are trying to obtain cost-optimal decisions, the network tries to determine the unknown parameters in a way that minimizes the average costs on our data.  Thus, the problem can be stated as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3trnHXwpI93"
   },
   "source": [
    "\\begin{equation}\r\n",
    "\\min_{w,b} \\frac{1}{n}\\sum_{i=1}^{n}\\bigl[cu(d_i-\\theta(x_i;w,b))^+ + co(\\theta(x_i;w,b)-d_i)^+\\bigl],\r\n",
    "\\tag{10}\r\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLMKccx7sRJh"
   },
   "source": [
    "where $\\theta(\\cdot)$ represents the function of the network with weights $w$ and biases $b$. Now look, this is again similar to equation $(6)$. The only difference is that $q(\\cdot)$ is represented by a DNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8ZDdC5_5bzK"
   },
   "source": [
    "To apply this approach on the Yaz dataset we can use the class `DeepLearningNewsvendor` from `ddop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOljfgX06UIG",
    "outputId": "4bd367a3-ea32-4fd7-b5cf-dd47786a381b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.17223325528597"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DLN = DeepLearningNewsvendor(cu=15,co=5)\r\n",
    "DLN.fit(X_train_scaled,y_train)\r\n",
    "-DLN.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIFC-57mnC9X"
   },
   "source": [
    "As we can see, the average cost are higher compared to both the linear regression based model and the WSAA models. One reason for this might be that a neural network often needs more data to learn a good decision function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQA0r0hAgL0l"
   },
   "source": [
    "### Summary\r\n",
    "\r\n",
    "In this part of the tutorial we have seen three different data-driven approaches to solve the newsvendor problem. \r\n",
    "\r\n",
    "* In the simplest case where we only have past demand observations we can use sample average approximation **(SAA)** to solve the newsvendor problem. The goal of **SAA** is to find the decision $q$ that minimizes the average costs on past demand samples.\r\n",
    "* However, we have seen that additional demand features can improve decision making, since they usually reduce the degree of uncertainty.\r\n",
    "* With weighted sample average approximation **(wSAA)** and empirical risk minimization **(ERM)** we got to know two data-driven approaches that can take such features into account by learning a function $q(\\cdot)$ that maps from features $x$ to a decision $q$. \r\n",
    "* **wSAA**, on the one hand, defines the function $q(\\cdot)$ point-wise. It is based on deriving sample weights from features and optimizing **SAA** against a re-weighting of the training data. To determine the weights we can use different weight functions, e.g., based on KNN or DT regression. \r\n",
    "* The **ERM** approach, on the other hand, tries to find the function $q(\\cdot)$ that maps from features to a decision by minimizing its empirical risk. Therefore, we have to specify the function space to which the decision function belongs. With `LinearRegressionNewsvendor`, and `DeepLearningNewsvendor` we have seen two models that define $q(\\cdot)$ in a different manner. While the former one assumes that $q(\\cdot)$ is a linear decision function, the latter one defines $q(\\cdot)$ as non-linear function by using as a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75c8RgZqBxcr"
   },
   "source": [
    "## A Final Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDaqjuN5CDee"
   },
   "source": [
    "In this tutorial, you learned how to solve the newsvendor problem based on past demand data. For the specific case of Yaz and the data for the product \"steak\", you have seen that several data-driven algorithms can outperform the traditional parametric approach. The best method has been shown to be **wSAA** with a weight function based on k-nearest-neighbors regression. However, it is important to note that there is no such thing as \"the best model\". Which model performs best can vary from one task to another. It may depend on the cost parameters, the number of features or the size of the dataset. This is exactly the reason why `ddop` provides easy access to a set of different models that can be used to find the best one for a given problem. As a next step, you can now build up on what you have learned and apply the different algorithms to some other data. For example, you can start by using the data for a product other than \"steak\", or even several products at once. Alternatively, you can use the bakery dataset that is also provided within `ddop`.\r\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XNvGez2XKm18",
    "TQA0r0hAgL0l"
   ],
   "name": "E-Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
